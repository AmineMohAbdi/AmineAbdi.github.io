{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ae55c9",
   "metadata": {},
   "source": [
    "# <center>***Text Emotions Classification***<center>\n",
    "\n",
    "<img src=\"..\\Text Emotions Classification\\Images\\TextEmotion.png\"/>\n",
    "    \n",
    "Text Emotions Classification is a nuanced process pivotal for understanding and discerning the emotional content conveyed within textual data. This practice holds practical implications, such as enhancing user experiences on platforms like smartphone keyboards, which suggest relevant emojis based on the emotional tone of the text.\n",
    "\n",
    ">>***Step 1: Data Collection***\n",
    "Commencing the journey involves collecting diverse textual data encompassing a range of emotional expressions. Sources for this data may include social media platforms, customer feedback, or literature repositories.   \n",
    "***Step 2: Data Preparation and Exploration***\n",
    "Following data collection, meticulous preparation and exploration are imperative. This encompasses tasks like data cleansing, normalization, and tokenization. Exploratory data analysis techniques are then employed to unveil patterns and nuances in emotional expression.   \n",
    "***Step 3: Model Building and Evaluation***  \n",
    "Machine learning models are subsequently trained on the prepared dataset to classify texts based on their emotional context. Various algorithms, including natural language processing (NLP) techniques like sentiment analysis or deep learning models like recurrent neural networks (RNNs), can be utilized. Model performance is evaluated using metrics such as accuracy, precision, and recall.   \n",
    "***Step 4: Application and Interpretation***\n",
    "Upon model training and evaluation, they can be applied to classify emotions in unseen textual data. This classification aids in extracting valuable insights for diverse applications, such as sentiment analysis in customer feedback, personalized recommendations, or content moderation.\n",
    "\n",
    "In essence, Text Emotions Classification equips stakeholders with the tools to decipher emotional nuances within textual data, thereby facilitating enhanced communication, user engagement, and decision-making processes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4e8a57",
   "metadata": {},
   "source": [
    "Text Emotions Classification\n",
    "Text emotions classification is the problem of natural language processing and text classification. Here we need to train a text classification model to classify the emotion of a text.\n",
    "\n",
    "To solve this problem, we need labelled data of texts and their emotions. I found an ideal dataset to solve this problem on Kaggle. You can download the dataset from [here](https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp).\n",
    "\n",
    "In the section below, I’ll take you through how to train a text classification model for the task of Text Emotions Classification using Machine Learning and the Python programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce9a4f",
   "metadata": {},
   "source": [
    "## ***Text Emotions Classification using Python***\n",
    "\n",
    "I’ll start by importing the necessary Python libraries and the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ab7d457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text Emotions\n",
      "0  i can go from feeling so hopeless to so damned...  sadness\n",
      "1   im grabbing a minute to post i feel greedy wrong    anger\n",
      "2  i am ever feeling nostalgic about the fireplac...     love\n",
      "3                               i am feeling grouchy    anger\n",
      "4  ive been feeling a little burdened lately wasn...  sadness\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"..\\\\Text Emotions Classification\\\\Data\\\\train.txt\", sep=';')\n",
    "data.columns = [\"Text\", \"Emotions\"]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff57136",
   "metadata": {},
   "source": [
    "As this is a problem of natural language processing, I’ll start by tokenizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac88ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data[\"Text\"].tolist()\n",
    "labels = data[\"Emotions\"].tolist()\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf075bb",
   "metadata": {},
   "source": [
    "Now we need to pad the sequences to the same length to feed them into a neural network. Here’s how we can pad the sequences of the texts to have the same length:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efcf0abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620b5fd",
   "metadata": {},
   "source": [
    "Now I’ll use the label encoder method to convert the classes from strings to a numerical representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b64fb166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dcb044",
   "metadata": {},
   "source": [
    "We are now going to One-hot encode the labels. One hot encoding refers to the transformation of categorical labels into a binary representation where each label is represented as a vector of all zeros except a single 1. This is necessary because machine learning algorithms work with numerical data. So here is how we can One-hot encode the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cc22c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the labels\n",
    "one_hot_labels = keras.utils.to_categorical(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f5a16b",
   "metadata": {},
   "source": [
    "## <center>***Text Emotions Classification Model***<center>    \n",
    "    \n",
    "Now we will split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ad40b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(padded_sequences, \n",
    "                                                one_hot_labels, \n",
    "                                                test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d676b18",
   "metadata": {},
   "source": [
    "Now let’s define a neural network architecture for our classification problem and use it to train a model to classify emotions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba7108ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "400/400 [==============================] - 17s 41ms/step - loss: 1.3627 - accuracy: 0.4663 - val_loss: 0.8590 - val_accuracy: 0.7025\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 17s 43ms/step - loss: 0.3360 - accuracy: 0.8949 - val_loss: 0.5478 - val_accuracy: 0.8144\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 18s 45ms/step - loss: 0.0572 - accuracy: 0.9840 - val_loss: 0.5695 - val_accuracy: 0.8244\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 17s 42ms/step - loss: 0.0247 - accuracy: 0.9954 - val_loss: 0.5943 - val_accuracy: 0.8263\n",
      "Epoch 5/10\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.0171 - accuracy: 0.9966 - val_loss: 0.5885 - val_accuracy: 0.8297\n",
      "Epoch 6/10\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.0164 - accuracy: 0.9966 - val_loss: 0.6475 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "400/400 [==============================] - 16s 41ms/step - loss: 0.0139 - accuracy: 0.9970 - val_loss: 0.6244 - val_accuracy: 0.8266\n",
      "Epoch 8/10\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.0105 - accuracy: 0.9975 - val_loss: 0.6620 - val_accuracy: 0.8234\n",
      "Epoch 9/10\n",
      "400/400 [==============================] - 17s 42ms/step - loss: 0.0116 - accuracy: 0.9973 - val_loss: 0.6576 - val_accuracy: 0.8216\n",
      "Epoch 10/10\n",
      "400/400 [==============================] - 16s 40ms/step - loss: 0.0101 - accuracy: 0.9973 - val_loss: 0.6944 - val_accuracy: 0.8247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b80477af0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, \n",
    "                    output_dim=128, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation=\"relu\"))\n",
    "model.add(Dense(units=len(one_hot_labels[0]), activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(xtrain, ytrain, epochs=10, batch_size=32, validation_data=(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bbd857",
   "metadata": {},
   "source": [
    "Now let’s take a sentence as an input text and see how the model performs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "392abefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 83ms/step\n",
      "['sadness']\n"
     ]
    }
   ],
   "source": [
    "input_text = \"She didn't come today because she lost her dog yestertay!\"\n",
    "\n",
    "# Preprocess the input text\n",
    "input_sequence = tokenizer.texts_to_sequences([input_text])\n",
    "padded_input_sequence = pad_sequences(input_sequence, maxlen=max_length)\n",
    "prediction = model.predict(padded_input_sequence)\n",
    "predicted_label = label_encoder.inverse_transform([np.argmax(prediction[0])])\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3ffec",
   "metadata": {},
   "source": [
    "So this is how you can use Machine Learning for the task of text emotion classification using the Python programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386e187",
   "metadata": {},
   "source": [
    "# <center>***Summary***<center>\n",
    "Text emotion classification is the problem of assigning emotion to a text by understanding the context and the emotion behind the text. One real-world example is the keyboard of an iPhone that recommends the most relevant emoji by understanding the text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
